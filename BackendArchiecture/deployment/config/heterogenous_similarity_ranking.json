{
    "smoker-status": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "mohs-hardness": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n",
    "bitcoin-price-prediction": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n",
    "heartbeat": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "webmd-reviews": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n",
    "cirrhosis-outcomes": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "software-defects": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "hotel-reviews": "(1) The overall design of the code is to train multiple models using different pre-trained transformer models and then use the trained models to make predictions on the test data. The predictions from each model are then combined using weighted averaging to generate the final submission.\n\n(2) The overall model architecture involves using pre-trained transformer models for text classification. The code uses different pre-trained transformer models such as \"microsoft/deberta-v2-xlarge\", \"microsoft/deberta-xlarge\", \"microsoft/deberta-large\", and \"microsoft/deberta-v3-large\". Each model is trained separately and used to make predictions on the test data. The predictions from each model are then combined using weighted averaging to generate the final submission.\n\n(3) The important hyperparameters in this code are:\n- `use_mixup`: A boolean parameter indicating whether to use mixup augmentation during training.\n- `forward_type`: A string parameter indicating the type of forward pass to use during training.\n- `use_token_types`: A boolean parameter indicating whether to use token types during training.\n- `use_layer_norm`: A boolean parameter indicating whether to use layer normalization during training.\n- `batch_size`: The batch size used during training.\n- `maxlen`: The maximum length of the input sequences.\n- `num_workers`: The number of workers used for data loading during training.\n- `weight`: The weight assigned to each model during the weighted averaging of predictions.\n- `config_path`: The path to the configuration file for the pre-trained transformer model.\n- `tokenizer_path`: The path to the tokenizer used for tokenizing the input sequences.\n- `is_pickle`: A boolean parameter indicating whether to use pickle for saving/loading data.\n- `device`: The device (CPU or GPU) used for training.\n- `model_paths`: The paths to the saved model checkpoints for each fold.\n- `oof_name`: The name of the out-of-fold (oof) predictions file.\n- `dataset_module`: The module containing the dataset-related functions.\n- `inference_module`: The module containing the inference-related functions.\n\n(4) The optimization objective is to minimize the loss function during training. The specific loss function used is not mentioned in the code.\n\n(5) The advanced machine learning technique used in this code is the use of pre-trained transformer models for text classification. These models have been trained on large amounts of text data and can be fine-tuned on specific tasks such as the Kaggle competition in this case.\n\n(6) Some important tricks that play a role in high performance include:\n- Using mixup augmentation during training to improve generalization.\n- Using different pre-trained transformer models and combining their predictions using weighted averaging to leverage the strengths of each model.\n- Using layer normalization to improve the stability and convergence of the training process.\n- Using a large batch size and maximum sequence length to capture more information from the input sequences.\n- Using multiple workers for data loading to speed up the training process.\n- Using pickle for saving/loading data to improve efficiency.\n- Using GPU acceleration if available to speed up the training process.\n- Using out-of-fold (oof) predictions to evaluate the performance of the models during training.",
    "electricity": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "detect-ai-generation": "(1) The overall design of the code is to train multiple models using different pre-trained transformer models and then use the trained models to make predictions on the test data. The predictions from each model are then combined using weighted averaging to generate the final submission.\n\n(2) The overall model architecture involves using pre-trained transformer models for text classification. The code uses different pre-trained transformer models such as \"microsoft/deberta-v2-xlarge\", \"microsoft/deberta-xlarge\", \"microsoft/deberta-large\", and \"microsoft/deberta-v3-large\". Each model is trained separately and used to make predictions on the test data. The predictions from each model are then combined using weighted averaging to generate the final submission.\n\n(3) The important hyperparameters in this code are:\n- `use_mixup`: A boolean parameter indicating whether to use mixup augmentation during training.\n- `forward_type`: A string parameter indicating the type of forward pass to use during training.\n- `use_token_types`: A boolean parameter indicating whether to use token types during training.\n- `use_layer_norm`: A boolean parameter indicating whether to use layer normalization during training.\n- `batch_size`: The batch size used during training.\n- `maxlen`: The maximum length of the input sequences.\n- `num_workers`: The number of workers used for data loading during training.\n- `weight`: The weight assigned to each model during the weighted averaging of predictions.\n- `config_path`: The path to the configuration file for the pre-trained transformer model.\n- `tokenizer_path`: The path to the tokenizer used for tokenizing the input sequences.\n- `is_pickle`: A boolean parameter indicating whether to use pickle for saving/loading data.\n- `device`: The device (CPU or GPU) used for training.\n- `model_paths`: The paths to the saved model checkpoints for each fold.\n- `oof_name`: The name of the out-of-fold (oof) predictions file.\n- `dataset_module`: The module containing the dataset-related functions.\n- `inference_module`: The module containing the inference-related functions.\n\n(4) The optimization objective is to minimize the loss function during training. The specific loss function used is not mentioned in the code.\n\n(5) The advanced machine learning technique used in this code is the use of pre-trained transformer models for text classification. These models have been trained on large amounts of text data and can be fine-tuned on specific tasks such as the Kaggle competition in this case.\n\n(6) Some important tricks that play a role in high performance include:\n- Using mixup augmentation during training to improve generalization.\n- Using different pre-trained transformer models and combining their predictions using weighted averaging to leverage the strengths of each model.\n- Using layer normalization to improve the stability and convergence of the training process.\n- Using a large batch size and maximum sequence length to capture more information from the input sequences.\n- Using multiple workers for data loading to speed up the training process.\n- Using pickle for saving/loading data to improve efficiency.\n- Using GPU acceleration if available to speed up the training process.\n- Using out-of-fold (oof) predictions to evaluate the performance of the models during training.",
    "weather": "Greetings to the Kaggle Community. In this message I want to tell you about my solution.\n\nThanks to Kaggle for providing free GPU and TPU resources to everyone. On my graphics card (1050 Ti) I would not have achieved those results.\nThanks to Google for the excellent tensorflow library.\nAll of my work was done in Kaggle Notebooks and relies on TensorFlow capabilities.\n\nThe key decisions that, in my opinion, led to a good result:\n1. Use a combination of transformer encoder and two BidirectionalLSTM layers.\n2. Use patches like VisualTransformer.\n3. Reduce the resolution of targets.\n\n*How does it work?*\n\nSuppose we have a tdcsfog sensor data series with AccV, AccML, AccAP columns and len of 5000.\n\nFirst, apply mean-std normalization to AccV, AccML, AccAP columns.\n\n```python\ndef sample_normalize(sample):\n\tmean = tf.math.reduce_mean(sample)\n\tstd = tf.math.reduce_std(sample)\n\tsample = tf.math.divide_no_nan(sample-mean, std)\n\n\treturn sample.numpy()\n```\nThen the series is zero-padded so that the final length is divisible by block_size = 15552  (or 12096 for defog). Now the series shape is (15552,  3). \n\nAnd create patches with the patch_size = 18 (or 14 for defog):\n\n```python\nseries # Example shape (15552, 3)\nseries = tf.reshape(series, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], 3)) # Example shape (864, 18, 3)\nseries = tf.reshape(series, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))  # Example shape (864, 54)\n```\n\nNow the series shape is (864,  54). It's a model input.\n\nWhat to do with the StartHesitation, Turn, Walking data? Same, but apply tf.reduce_max at the end.\n\n```python\nseries_targets # Example shape (15552,  3)\nseries_targets = tf.reshape(series_targets, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], 3)) # Example shape (864, 18, 3)\nseries_targets = tf.transpose(series_targets, perm=[0, 2, 1]) # Example shape (864, 3, 18)\nseries_targets = tf.reduce_max(series_targets, axis=-1) # Example shape (864, 3)\n```\n\nNow the series shape is (864, 3). It's a model output.\n\nAt the end, simply return the true resolution with tf.tile\n\n```python\npredictions = model.predict(...) # Example shape (1, 864, 3)\npredictions = tf.expand_dims(predictions, axis=-1) # Example shape (1, 864, 3, 1)\npredictions = tf.transpose(predictions, perm=[0, 1, 3, 2]) # Example shape (1, 864, 1, 3)\npredictions = tf.tile(predictions, multiples=[1, 1, CFG['patch_size'], 1]) # Example shape (1, 864, 18, 3)\npredictions = tf.reshape(predictions, shape=(predictions.shape[0], predictions.shape[1]*predictions.shape[2], 3)) # Example shape (1, 15552, 3)\n```\n# Details\n\nDaily data, events.csv, subjects.csv, tasks.csv have never been used.\n\nTdcsfog data is not used to train defog models. \n\nDefog data is not used to train tdcsfog models.\n\n*Optimizer* \n\n```python\ntf.keras.optimizers.Adam(learning_rate=Schedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n```\n\n*Loss function*\n\n```python\n'''\nloss_function args exp\n\nreal is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 5) where the last axis means:\n0 - StartHesitation\n1 - Turn\n2 - Walking\n3 - Valid\n4 - Mask\n\noutput is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 3) where the last axis means:\n0 - StartHesitation predicted\n1 - Turn predicted\n2 - Walking predicted\n\n'''\n\nce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n\ndef loss_function(real, output, name='loss_function'):\n\tloss = ce(tf.expand_dims(real[:, :, 0:3], axis=-1), tf.expand_dims(output, axis=-1)) # Example shape (32, 864, 3)\n\n\tmask = tf.math.multiply(real[:, :, 3], real[:, :, 4]) # Example shape (32, 864)\n\tmask = tf.cast(mask, dtype=loss.dtype)\n\tmask = tf.expand_dims(mask, axis=-1) # Example shape (32, 864, 1)\n\tmask = tf.tile(mask, multiples=[1, 1, 3]) # Example shape (32, 864, 3)\n\tloss *= mask # Example shape (32, 864, 3)\n\n\treturn tf.reduce_sum(loss) / tf.reduce_sum(mask)\n```\n*Model* \n\n```python\nCFG = {'TPU': 0,\n\t'block_size': 15552,\n\t'block_stride': 15552//16,\n\t'patch_size': 18,\n\t \n\t'fog_model_dim': 320,\n\t'fog_model_num_heads': 6,\n\t'fog_model_num_encoder_layers': 5,\n\t'fog_model_num_lstm_layers': 2,\n\t'fog_model_first_dropout': 0.1,\n\t'fog_model_encoder_dropout': 0.1,\n\t'fog_model_mha_dropout': 0.0,\n\t}\n\n'''\nThe transformer encoder layer\nFor more details, see  [Attention Is All You Need]\n\n'''\n\nclass EncoderLayer(tf.keras.layers.Layer):\n\tdef __init__(self):\n\tsuper().__init__()\n\t \n\tself.mha = tf.keras.layers.MultiHeadAttention(num_heads=CFG['fog_model_num_heads'], key_dim=CFG['fog_model_dim'], dropout=CFG['fog_model_mha_dropout'])\n\t \n\tself.add = tf.keras.layers.Add()\n\t \n\tself.layernorm = tf.keras.layers.LayerNormalization()\n\t \n\tself.seq = tf.keras.Sequential([tf.keras.layers.Dense(CFG['fog_model_dim'], activation='relu'),\n\ttf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']),\n\ttf.keras.layers.Dense(CFG['fog_model_dim']),\n\ttf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']),\n\t])\n\t \n\tdef call(self, x):\n\tattn_output = self.mha(query=x, key=x, value=x)\n\tx = self.add([x, attn_output])\n\tx = self.layernorm(x)\n\tx = self.add([x, self.seq(x)])\n\tx = self.layernorm(x)\n\t \n\treturn x\n\n'''\nFOGEncoder is a combination of transformer encoder (D=320, H=6, L=5) and two BidirectionalLSTM layers\n\n'''\n\nclass FOGEncoder(tf.keras.Model):\n\tdef __init__(self):\n\tsuper().__init__()\n\t \n\tself.first_linear = tf.keras.layers.Dense(CFG['fog_model_dim'])\n\t \n\tself.add = tf.keras.layers.Add()\n\t \n\tself.first_dropout = tf.keras.layers.Dropout(CFG['fog_model_first_dropout'])\n\t \n\tself.enc_layers = [EncoderLayer() for _ in range(CFG['fog_model_num_encoder_layers'])]\n\t \n\tself.lstm_layers = [tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(CFG['fog_model_dim'], return_sequences=True)) for _ in range(CFG['fog_model_num_lstm_layers'])]\n\t \n\tself.sequence_len = CFG['block_size'] // CFG['patch_size']\n\tself.pos_encoding = tf.Variable(initial_value=tf.random.normal(shape=(1, self.sequence_len, CFG['fog_model_dim']), stddev=0.02), trainable=True)\n\t \n\tdef call(self, x, training=None): # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), Example shape (4, 864, 54)\n\tx = x / 25.0 # Normalization attempt in the segment [-1, 1]\n\tx = self.first_linear(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']), Example shape (4, 864, 320)\n\t \n\tif training: # augmentation by randomly roll of the position encoding tensor\n\trandom_pos_encoding = tf.roll(tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1]),\n\tshift=tf.random.uniform(shape=(GPU_BATCH_SIZE,), minval=-self.sequence_len, maxval=0, dtype=tf.int32),\n\taxis=GPU_BATCH_SIZE * [1],\n\t)\n\tx = self.add([x, random_pos_encoding])\n\t \n\telse: # without augmentation\n\tx = self.add([x, tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1])])\n\t \n\tx = self.first_dropout(x)\n\t \n\tfor i in range(CFG['fog_model_num_encoder_layers']): x = self.enc_layers[i](x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']), Example shape (4, 864, 320)\n\tfor i in range(CFG['fog_model_num_lstm_layers']): x = self.lstm_layers[i](x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']*2), Example shape (4, 864, 640)\n\t \n\treturn x\n\nclass FOGModel(tf.keras.Model):\n\tdef __init__(self):\n\tsuper().__init__()\n\t \n\tself.encoder = FOGEncoder()\n\tself.last_linear = tf.keras.layers.Dense(3)\n\t \n\tdef call(self, x): # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), Example shape (4, 864, 54)\n\tx = self.encoder(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']*2), Example shape (4, 864, 640)\n\tx = self.last_linear(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 3), Example shape (4, 864, 3)\n\tx = tf.nn.sigmoid(x) # Sigmoid activation\n\t \n\treturn x\n\n```\n\n\n\n# Submission (Private Score 0.514, Public Score 0.527) consists of 8 models:\n\n### Model 1 (tdcsfog model)\n\n```python\nCFG = {'TPU': 1, \n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/38\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE=32\n```\n\nValidation subjects \n['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n\nTrain 15 minutes on TPU. Validation scores:\nStartHesitation AP - 0.462 Turn AP - 0.896 Walking AP - 0.470 mAP - 0.609\n\n### Model 2 (tdcsfog model)\n\n```python\nCFG = {'TPU': 0, \n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 256,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 3,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/24\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE = 16\n```\n\nValidation subjects \n['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n\nTrain 40 minutes on GPU. Validation scores:\nStartHesitation AP - 0.481 Turn AP - 0.886 Walking AP - 0.437 mAP - 0.601\n\n### Model 3 (tdcsfog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/48\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE = 32\n```\n\nValidation subjects \n['e39bc5', '516a67', 'af82b2', '4dc2f8', '743f4e', 'fa8764', 'a03db7', '51574c', '2d57c2']\n\nTrain 11 minutes on TPU. Validation scores:\nStartHesitation AP - 0.601 Turn AP - 0.857 Walking AP - 0.289 mAP - 0.582\n\n### Model 4 (tdcsfog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/38\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE = 32\n```\n\nValidation subjects \n['5c0b8a', 'a03db7', '7fcee9', '2c98f7', '2a39f8', '4f13b4', 'af82b2', 'f686f0', '93f49f', '194d1d', '02bc69', '082f01']\n\nTrain 13 minutes on TPU. Validation scores:\nStartHesitation AP - 0.367 Turn AP - 0.879 Walking AP - 0.194 mAP - 0.480\n\n### Model 5 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/62\nSTEPS_PER_EPOCH = 256\nWARMUP_STEPS = 256\nBATCH_SIZE = 32\n```\n\nValidation subjects \n['00f674', '8d43d9', '107712', '7b2e84', '575c60', '7f8949', '2874c5', '72e2c7']\n\nTrain data: defog data, notype data\nValidation data: defog data, notype data\n\nTrain 45 minutes on TPU. Validation scores:\nStartHesitation AP - [not used] Turn AP - 0.625 Walking AP - 0.238 mAP - 0.432\nEvent AP - 0.800\n\n### Model 6 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 5,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n```\n\nTrain data: defog data (about 85%)\nValidation data: defog data (about 15%), notype data (100%)\n\n### Model 7 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 4,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/24\nSTEPS_PER_EPOCH = 32\nWARMUP_STEPS = 64\nBATCH_SIZE = 128\n```\n\nTrain data: defog data (100%)\nValidation data: notype data (100%)\n\nTrain 18 minutes on TPU. Validation scores:\nStartHesitation AP - [not used] Turn AP - [not used] Walking AP - [not used] mAP - [not used]\nEvent AP - 0.764\n\n### Model 8 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/46\nSTEPS_PER_EPOCH = 256\nWARMUP_STEPS = 256\nBATCH_SIZE = 32\n```\n\nValidation subjects\n['12f8d1', '8c1f5e', '387ea0', 'c56629', '7da72f', '413532', 'd89567', 'ab3b2e', 'c83ff6', '056372']\n\nTrain data: defog data, notype data\nValidation data: defog data, notype data\n\nTrain 28 minutes on TPU. Validation scores:\nStartHesitation AP - [not used] Turn AP - 0.758 Walking AP - 0.221 mAP - 0.489\nEvent AP - 0.744\n\n# Final models\n\nTdcsfog:  0.25 * Model 1 + 0.25 * Model 2 + 0.25 * Model 3 + 0.25 * Model 4\n\nDefog: 0.25 * Model 5 + 0.25 * Model 6 + 0.25 * Model 7 + 0.25 * Model 8\n\n",
    "self-regulation-scp1": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "uwave-gesture-library": "Greetings to the Kaggle Community. In this message I want to tell you about my solution.\n\nThanks to Kaggle for providing free GPU and TPU resources to everyone. On my graphics card (1050 Ti) I would not have achieved those results.\nThanks to Google for the excellent tensorflow library.\nAll of my work was done in Kaggle Notebooks and relies on TensorFlow capabilities.\n\nThe key decisions that, in my opinion, led to a good result:\n1. Use a combination of transformer encoder and two BidirectionalLSTM layers.\n2. Use patches like VisualTransformer.\n3. Reduce the resolution of targets.\n\n*How does it work?*\n\nSuppose we have a tdcsfog sensor data series with AccV, AccML, AccAP columns and len of 5000.\n\nFirst, apply mean-std normalization to AccV, AccML, AccAP columns.\n\n```python\ndef sample_normalize(sample):\n\tmean = tf.math.reduce_mean(sample)\n\tstd = tf.math.reduce_std(sample)\n\tsample = tf.math.divide_no_nan(sample-mean, std)\n\n\treturn sample.numpy()\n```\nThen the series is zero-padded so that the final length is divisible by block_size = 15552  (or 12096 for defog). Now the series shape is (15552,  3). \n\nAnd create patches with the patch_size = 18 (or 14 for defog):\n\n```python\nseries # Example shape (15552, 3)\nseries = tf.reshape(series, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], 3)) # Example shape (864, 18, 3)\nseries = tf.reshape(series, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3))  # Example shape (864, 54)\n```\n\nNow the series shape is (864,  54). It's a model input.\n\nWhat to do with the StartHesitation, Turn, Walking data? Same, but apply tf.reduce_max at the end.\n\n```python\nseries_targets # Example shape (15552,  3)\nseries_targets = tf.reshape(series_targets, shape=(CFG['block_size'] // CFG['patch_size'], CFG['patch_size'], 3)) # Example shape (864, 18, 3)\nseries_targets = tf.transpose(series_targets, perm=[0, 2, 1]) # Example shape (864, 3, 18)\nseries_targets = tf.reduce_max(series_targets, axis=-1) # Example shape (864, 3)\n```\n\nNow the series shape is (864, 3). It's a model output.\n\nAt the end, simply return the true resolution with tf.tile\n\n```python\npredictions = model.predict(...) # Example shape (1, 864, 3)\npredictions = tf.expand_dims(predictions, axis=-1) # Example shape (1, 864, 3, 1)\npredictions = tf.transpose(predictions, perm=[0, 1, 3, 2]) # Example shape (1, 864, 1, 3)\npredictions = tf.tile(predictions, multiples=[1, 1, CFG['patch_size'], 1]) # Example shape (1, 864, 18, 3)\npredictions = tf.reshape(predictions, shape=(predictions.shape[0], predictions.shape[1]*predictions.shape[2], 3)) # Example shape (1, 15552, 3)\n```\n# Details\n\nDaily data, events.csv, subjects.csv, tasks.csv have never been used.\n\nTdcsfog data is not used to train defog models. \n\nDefog data is not used to train tdcsfog models.\n\n*Optimizer* \n\n```python\ntf.keras.optimizers.Adam(learning_rate=Schedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n```\n\n*Loss function*\n\n```python\n'''\nloss_function args exp\n\nreal is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 5) where the last axis means:\n0 - StartHesitation\n1 - Turn\n2 - Walking\n3 - Valid\n4 - Mask\n\noutput is a tensor with the shape (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 3) where the last axis means:\n0 - StartHesitation predicted\n1 - Turn predicted\n2 - Walking predicted\n\n'''\n\nce = tf.keras.losses.BinaryCrossentropy(reduction='none')\n\ndef loss_function(real, output, name='loss_function'):\n\tloss = ce(tf.expand_dims(real[:, :, 0:3], axis=-1), tf.expand_dims(output, axis=-1)) # Example shape (32, 864, 3)\n\n\tmask = tf.math.multiply(real[:, :, 3], real[:, :, 4]) # Example shape (32, 864)\n\tmask = tf.cast(mask, dtype=loss.dtype)\n\tmask = tf.expand_dims(mask, axis=-1) # Example shape (32, 864, 1)\n\tmask = tf.tile(mask, multiples=[1, 1, 3]) # Example shape (32, 864, 3)\n\tloss *= mask # Example shape (32, 864, 3)\n\n\treturn tf.reduce_sum(loss) / tf.reduce_sum(mask)\n```\n*Model* \n\n```python\nCFG = {'TPU': 0,\n\t'block_size': 15552,\n\t'block_stride': 15552//16,\n\t'patch_size': 18,\n\t \n\t'fog_model_dim': 320,\n\t'fog_model_num_heads': 6,\n\t'fog_model_num_encoder_layers': 5,\n\t'fog_model_num_lstm_layers': 2,\n\t'fog_model_first_dropout': 0.1,\n\t'fog_model_encoder_dropout': 0.1,\n\t'fog_model_mha_dropout': 0.0,\n\t}\n\n'''\nThe transformer encoder layer\nFor more details, see  [Attention Is All You Need]\n\n'''\n\nclass EncoderLayer(tf.keras.layers.Layer):\n\tdef __init__(self):\n\tsuper().__init__()\n\t \n\tself.mha = tf.keras.layers.MultiHeadAttention(num_heads=CFG['fog_model_num_heads'], key_dim=CFG['fog_model_dim'], dropout=CFG['fog_model_mha_dropout'])\n\t \n\tself.add = tf.keras.layers.Add()\n\t \n\tself.layernorm = tf.keras.layers.LayerNormalization()\n\t \n\tself.seq = tf.keras.Sequential([tf.keras.layers.Dense(CFG['fog_model_dim'], activation='relu'),\n\ttf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']),\n\ttf.keras.layers.Dense(CFG['fog_model_dim']),\n\ttf.keras.layers.Dropout(CFG['fog_model_encoder_dropout']),\n\t])\n\t \n\tdef call(self, x):\n\tattn_output = self.mha(query=x, key=x, value=x)\n\tx = self.add([x, attn_output])\n\tx = self.layernorm(x)\n\tx = self.add([x, self.seq(x)])\n\tx = self.layernorm(x)\n\t \n\treturn x\n\n'''\nFOGEncoder is a combination of transformer encoder (D=320, H=6, L=5) and two BidirectionalLSTM layers\n\n'''\n\nclass FOGEncoder(tf.keras.Model):\n\tdef __init__(self):\n\tsuper().__init__()\n\t \n\tself.first_linear = tf.keras.layers.Dense(CFG['fog_model_dim'])\n\t \n\tself.add = tf.keras.layers.Add()\n\t \n\tself.first_dropout = tf.keras.layers.Dropout(CFG['fog_model_first_dropout'])\n\t \n\tself.enc_layers = [EncoderLayer() for _ in range(CFG['fog_model_num_encoder_layers'])]\n\t \n\tself.lstm_layers = [tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(CFG['fog_model_dim'], return_sequences=True)) for _ in range(CFG['fog_model_num_lstm_layers'])]\n\t \n\tself.sequence_len = CFG['block_size'] // CFG['patch_size']\n\tself.pos_encoding = tf.Variable(initial_value=tf.random.normal(shape=(1, self.sequence_len, CFG['fog_model_dim']), stddev=0.02), trainable=True)\n\t \n\tdef call(self, x, training=None): # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), Example shape (4, 864, 54)\n\tx = x / 25.0 # Normalization attempt in the segment [-1, 1]\n\tx = self.first_linear(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']), Example shape (4, 864, 320)\n\t \n\tif training: # augmentation by randomly roll of the position encoding tensor\n\trandom_pos_encoding = tf.roll(tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1]),\n\tshift=tf.random.uniform(shape=(GPU_BATCH_SIZE,), minval=-self.sequence_len, maxval=0, dtype=tf.int32),\n\taxis=GPU_BATCH_SIZE * [1],\n\t)\n\tx = self.add([x, random_pos_encoding])\n\t \n\telse: # without augmentation\n\tx = self.add([x, tf.tile(self.pos_encoding, multiples=[GPU_BATCH_SIZE, 1, 1])])\n\t \n\tx = self.first_dropout(x)\n\t \n\tfor i in range(CFG['fog_model_num_encoder_layers']): x = self.enc_layers[i](x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']), Example shape (4, 864, 320)\n\tfor i in range(CFG['fog_model_num_lstm_layers']): x = self.lstm_layers[i](x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']*2), Example shape (4, 864, 640)\n\t \n\treturn x\n\nclass FOGModel(tf.keras.Model):\n\tdef __init__(self):\n\tsuper().__init__()\n\t \n\tself.encoder = FOGEncoder()\n\tself.last_linear = tf.keras.layers.Dense(3)\n\t \n\tdef call(self, x): # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['patch_size']*3), Example shape (4, 864, 54)\n\tx = self.encoder(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], CFG['fog_model_dim']*2), Example shape (4, 864, 640)\n\tx = self.last_linear(x) # (GPU_BATCH_SIZE, CFG['block_size'] // CFG['patch_size'], 3), Example shape (4, 864, 3)\n\tx = tf.nn.sigmoid(x) # Sigmoid activation\n\t \n\treturn x\n\n```\n\n\n\n# Submission (Private Score 0.514, Public Score 0.527) consists of 8 models:\n\n### Model 1 (tdcsfog model)\n\n```python\nCFG = {'TPU': 1, \n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/38\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE=32\n```\n\nValidation subjects \n['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n\nTrain 15 minutes on TPU. Validation scores:\nStartHesitation AP - 0.462 Turn AP - 0.896 Walking AP - 0.470 mAP - 0.609\n\n### Model 2 (tdcsfog model)\n\n```python\nCFG = {'TPU': 0, \n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 256,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 3,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/24\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE = 16\n```\n\nValidation subjects \n['07285e', '220a17', '54ee6e', '312788', '24a59d', '4bb5d0', '48fd62', '79011a', '7688c1']\n\nTrain 40 minutes on GPU. Validation scores:\nStartHesitation AP - 0.481 Turn AP - 0.886 Walking AP - 0.437 mAP - 0.601\n\n### Model 3 (tdcsfog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/48\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE = 32\n```\n\nValidation subjects \n['e39bc5', '516a67', 'af82b2', '4dc2f8', '743f4e', 'fa8764', 'a03db7', '51574c', '2d57c2']\n\nTrain 11 minutes on TPU. Validation scores:\nStartHesitation AP - 0.601 Turn AP - 0.857 Walking AP - 0.289 mAP - 0.582\n\n### Model 4 (tdcsfog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 15552, \n'block_stride': 15552//16,\n'patch_size': 18, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/38\nSTEPS_PER_EPOCH = 64\nWARMUP_STEPS = 64\nBATCH_SIZE = 32\n```\n\nValidation subjects \n['5c0b8a', 'a03db7', '7fcee9', '2c98f7', '2a39f8', '4f13b4', 'af82b2', 'f686f0', '93f49f', '194d1d', '02bc69', '082f01']\n\nTrain 13 minutes on TPU. Validation scores:\nStartHesitation AP - 0.367 Turn AP - 0.879 Walking AP - 0.194 mAP - 0.480\n\n### Model 5 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/62\nSTEPS_PER_EPOCH = 256\nWARMUP_STEPS = 256\nBATCH_SIZE = 32\n```\n\nValidation subjects \n['00f674', '8d43d9', '107712', '7b2e84', '575c60', '7f8949', '2874c5', '72e2c7']\n\nTrain data: defog data, notype data\nValidation data: defog data, notype data\n\nTrain 45 minutes on TPU. Validation scores:\nStartHesitation AP - [not used] Turn AP - 0.625 Walking AP - 0.238 mAP - 0.432\nEvent AP - 0.800\n\n### Model 6 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 5,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n```\n\nTrain data: defog data (about 85%)\nValidation data: defog data (about 15%), notype data (100%)\n\n### Model 7 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 4,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/24\nSTEPS_PER_EPOCH = 32\nWARMUP_STEPS = 64\nBATCH_SIZE = 128\n```\n\nTrain data: defog data (100%)\nValidation data: notype data (100%)\n\nTrain 18 minutes on TPU. Validation scores:\nStartHesitation AP - [not used] Turn AP - [not used] Walking AP - [not used] mAP - [not used]\nEvent AP - 0.764\n\n### Model 8 (defog model)\n\n```python\nCFG = {'TPU': 1,\n'block_size': 12096, \n'block_stride': 12096//16,\n'patch_size': 14, \n\n'fog_model_dim': 320,\n'fog_model_num_heads': 6,\n'fog_model_num_encoder_layers': 5,\n'fog_model_num_lstm_layers': 2,\n'fog_model_first_dropout': 0.1,\n'fog_model_encoder_dropout': 0.1,\n'fog_model_mha_dropout': 0.0,\n}\n\nLEARNING_RATE = 0.01/46\nSTEPS_PER_EPOCH = 256\nWARMUP_STEPS = 256\nBATCH_SIZE = 32\n```\n\nValidation subjects\n['12f8d1', '8c1f5e', '387ea0', 'c56629', '7da72f', '413532', 'd89567', 'ab3b2e', 'c83ff6', '056372']\n\nTrain data: defog data, notype data\nValidation data: defog data, notype data\n\nTrain 28 minutes on TPU. Validation scores:\nStartHesitation AP - [not used] Turn AP - 0.758 Walking AP - 0.221 mAP - 0.489\nEvent AP - 0.744\n\n# Final models\n\nTdcsfog:  0.25 * Model 1 + 0.25 * Model 2 + 0.25 * Model 3 + 0.25 * Model 4\n\nDefog: 0.25 * Model 5 + 0.25 * Model 6 + 0.25 * Model 7 + 0.25 * Model 8\n\n",
    "traffic": "Thanks to the organizer and the Kaggle team for hosting this competition. And thanks to many participants who shared their ideas with notebook or discussion. It's difficult to improve the score until we find the \"magic\". Fortunately, our team make the breakthrough and get 3rd place at the end of the competition. Great thanks to my teammates and their hard work! @xiamaozi11 @renxingkai @decalogue \n\n## Summary\n\nOur team tried to find the additional information about anchor and target in the [public dataset]( shared by the organizer. However, this method has a little benefit because only part of them are matched or those texts are useless.\n\nThe essential part of our solution is adding targets with the same anchor to each data sample. This data processing trick boosts our score from 0.84x to 0.85x on LB by a single model.\n\nWe stack 12 different models in the final submission. DeBERTa V3 large with MSE loss gives the best single model score on both CV and LB.\n\n\n## Validation strategy\n\nBoth `StratifiedGroupKFold` and  `GroupKFold` can prevent data with the same anchor from leaking to validation set. `GroupKFold` can keep the same training data size of each fold, while `StratifiedGroupKFold` can keep the distribution of the label. Both of them are used (by different team member) and get relatively strong correlation between CV and LB.\n\n\n## Data processing\n\nInput data from baseline\n```\nanchor [SEP] target [SEP] context text\n```\n\nOur input data\n```\nanchor [SEP] target; target_x1; target_x2; ... traget_xn; [SEP] context text\n```\nwhere target_xi are targets with the same anchor and context code.\n\nIt's easy to get comaprable improvement by hard encoding them while shuffling the sequence can reach higher score.\n\n\n## Model\n\nPretrained model\n- Electra large\n- Bert For Patent\n- DeBERTa V3 large\n- DeBERTa V1\n- DeBERTa V1 xlarge\n\nLoss\n- binary cross entropy loss\n- mean squared error loss\n- pearson correlation loss\n\nThere is no big difference among those loss functions. However, using different loss in training phrases will lead to high diversity when ensembling because the distribution of the prediction looks different from oof.\n\nTricks\n- different learning rate for different layer\n- fgm\n- ema\n\nYou may get around 1k~2k improvement by adding all of those tricks.\n\n## Result\n\nSingle Model\n\n| Model             | CV     | Public Score | Private Score |\n| ----------------- | ------ | ------------ | ------------- |\n| Bert For Patent   | 0.8362 | /            | /             |\n| DeBERTa V3 large  | 0.8516 | 0.8559       | 0.8675        |\n| DeBERTa V1        | 0.8385 | /            | /             |\n| DeBERTa V1 xlarge | 0.8423 | /            | /             |\n| Electra large     | 0.8483 | /            | /             |\n\nEnsemble\n\n12 models with different cross validation strategy, different concatenating methods, different pretrained models and different loss function.\n\n| Model              | CV     | Public Score | Private Score |\n| ------------------ | ------ | ------------ | ------------- |\n| Mean of 12 models  | 0.8674 | 0.8627       | 0.8765        |\n| Stacking 12 models | 0.8683 | 0.8640       | 0.8772        |\n\n\n## Other ideas\n\nThere are some ideas we think useful but have no time to try\n\n- Pretrained with the cpc text\n- Prompt learning\n- Predict the score of those concatenated targets together\n\n",
    "boolq": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n",
    "crab-age": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n",
    "concrete-strength": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n",
    "jigsaw": "First of all, I would like to thank kaggle and the staff for hosting such an interesting competition.\nAlso, I really appreciate my teammates, @harshit92, @ynishizono, @muhammad4hmed Congratulations to become the competition master and @trushk Congratulations to 2nd gold medal !\n\n\n# 1. Summary (Our Magic and got single model public LB : 0.8562, private : 0.8717)\nOur magic was to group the target words per \"anchor + context\" and attach them to the end of each sentence.Maybe it's easier to understand by looking at the code, so I'll share it.\n\n```\ntrain['group'] = train['context'] + \" \" + train['anchor']\n\nallres = {}\n\nfor text in tqdm(train[\"group\"].unique()):\ntmpdf = train[train[\"group\"]==text].reset_index(drop=True)\ntexts = \",\".join(tmpdf[\"target\"])\nallres[text] = texts\n\ntrain[\"target_gp\"] = train[\"group\"].map(allres)\n\ntrain[\"input\"] = train.anchor + \" \" + tokenizer.sep_token + \" \" + train.target + \" \" + tokenizer.sep_token + \" \" + train.title + \" \" + tokenizer.sep_token + \" \" + train.target_gp\n\n```\nfor example, we get like this sentence as input. And training.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nabatement [SEP] abatement of pollution [SEP] HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL [SEP] abatement of pollution,act of abating,active catalyst,eliminating process,forest region,greenhouse gases,increased rate,measurement level,minimising sounds,mixing core materials,multi pollution abatement device,noise reduction,pollution abatement,pollution abatement incinerator,pollution certificate,rent abatement,sorbent material,source items pollution abatement technology,stone abutments,tax abatement,water bodies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy doing so, we thought that we could not only train one sentence, but also train considering the correlation of the target words for each \"anchor + context\" with attention. Moreover, in order to avoid leakage, groupkfold of \"anchor + context\" was adopted. As a result, this magic boosted our models (in best case, public lb 0.8418 \u2192 0.8562) two days before closing.\n\nThis idea was decisive for getting into the gold medal zone. (Only this single model we can get the gold)\n\n------------------Details below---------------------------------------\n\n# 2. Preprocess and cross validation\n\npreprocess and cross validation is proposed by @harshit92 Basically, we used the lower case, not using [SEP] but uses [sep], remove \";\" , \",\" , and \".\" like this.\n\n~~~\ntrain['input'] = train['anchor'] + ' [SEP] ' + train['target'] + ' [SEP] '  + train['context_text']\ntrain['input'] = train['input'].apply(lambda x: x.lower())\ntrain['input'] = train['input'].apply(lambda x: x.replace(';','').replace(',','').replace('.',''))\n~~~\n\nAnd he found the boosting by changing 5kfold to 10kfold as NBME 1st solution\n(public LB : 5kfold 0.8395 \u2192 10kfold 0.8435). These were the strong tools for us.\n\n# 3. Model making\n\n## 3.1 How to catch the problem\nWe did not just solve the 1 target prediction, but to make it more diverse, we solved the problem as follows.\n\n### 3.1.1 BCE with binning\nThe score value was replaced as follows. And sigmoid was calculated in each predictions and averaged.\n\n~~~\n0:[0,0,0,0], 0.25:[1,0,0,0], 0.5:[1,1,0,0], 0.75:[1,1,1,0],1:[1,1,1,1]\noutput = sigmoid in each prediction and averaged\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4], output = (0.1 + 0.3 + 0.2 + 0.4) /4\n\n~~~\n\n### 3.1.2 Softmax with binning\nThe score was replaced as follows. And softmax was calculated in each predictions and convoluted.\n\n~~~\n0:[0, 0, 0, 0, 0], 0.25:[0,1,0,0,0], 0.5:[0,0,1,0,0], 0.75:[0,0,0,1,0],1:[0,0,0,0,1]\noutput = softmax in each prediction and convolution operation\n\nFor example, prediction = [0.1, 0.3, 0.2, 0.4, 0], output = 0*0.1 + 0.25*0.3 + 0.5*0.2 + 0.75*0.4 +1.0*0\n\n~~~\n\n## 3.2 AWP\n\nAs with Feedback and NBME, we were able to improve our score with AWP.\nI got the code from the following in Feedback [code](\n\n\nThis boosted my model public LB : 0.8394  to 0.8418\n\n## 3.3 Other tips that worked well\n\n- Mixout by @trushk \n- Knowledge distillation(KD) by @ynishizono \n- text embedding with SVR \n- mix various loss (ex. MSE + Corr)\n- dynamic padding for some arches \n\n## 3.4 Didn't work well\n\n- MLM\n- pseudo labeling (export all combination of anchor and target per context)\n- Random noise of [MASK]\n- change the order of input\n- post process of Gradiant Boost\n- adding per subsection title (some case is better.)\n- concat text embedding and SVR like PetFinder 1st solution\n\n# 4. Ensemble\nFor our ensemble, we used the nelder-mead coefficient by oof files. Candidates were automatically exported by @trushk 's code which uses the mix of the hill climb and nelder-mead algorithm. Finally, the used models were extracted based on over 90 oof files, and adjusted manually.\n\nThere are the list of models for the final submission. \n\n| model id | model                        | Feature   | Task            | Magic | kfold | cv      | public LB  | private LB | weight |\n|----------|------------------------------|-----------|-----------------|-------|-------|---------|------------|------------|--------|\n| 1        | deberta-v3-large             | AWP       | MSE             | TRUE  | 15    | 0.8605  | 0.8562     | 0.8717     | 0.237  |\n| 2        | electra-large-discriminator  |           | MSE             | TRUE  | 15    | 0.8456  | 0.8406     | 0.8534     | 0.166  |\n| 3        | electra-large-discriminator  |           | MSE             |       | 15    | 0.8381  | 0.8339     | 0.8486     | 0.049  |\n| 4        | bert-for-patents             | KD + SVR  |  BCE binning    |       | 5     | 0.8339  |            |            | 0.087  |\n| 5        | deberta-v3-large             | KD + SVR  | MSE             |       | 5     | 0.8470  |            |            | 0.129  |\n| 6        | deberta-v3-large             |           |  BCE binning    | TRUE  | 5     | 0.8471  | 0.8512     | 0.8664     | 0.067  |\n| 7        | deberta-v3-large             | Mixout    | Softmax binning | TRUE  | 5     | 0.8440  | 0.8506     | 0.8644     | 0.057  |\n| 8        | bert-for-patents             | Mixout    | Softmax binning | TRUE  | 5     | 0.8340  |            |            | 0.084  |\n| 9        | deberta-v3-large             |           |  BCE binning    | TRUE  | 10    | 0.8463  |            |            | 0.092  |\n| 10       | deberta-v3-large             |           |  BCE binning    |       | 10    | 0.8335  | 0.8390     | 0.8579     | 0.073  |\n\n\n\nFinal our cv is 0.8721, public lb is 0.8604, private lb is 0.8750 (11th).\n\nAs reference, this is all of our cv and lb relationship. The difference in color indicates the difference between people. We discussed based on this. \n\n![lb](\n\n\n# 5. Acknowledgments\n\nWe couldn't get this score on our own. Thank you to everyone who shared past knowledge and code! We respect to you. \n\nAnd I think the reason we got the gold medal was largely due to the sharing and discussion of the daily results. Eventually it leaded to the magic. We are the best team ! Thank you !!\n\nFrom our team :\n\n![Our team](\n\n"
}